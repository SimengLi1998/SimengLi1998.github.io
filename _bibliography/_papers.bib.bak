---
---

@article{li2024owlore,
  title={OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning},
  author={Li, Pengxiang and Yin, Lu and Gao, Xiaowei and Liu, Shiwei},
  journal={arXiv preprint arXiv:2405.18380},
    abbr = {Preprint},
    selected={true},
  year={2024}
}

@article{zhang2024q,
  title={Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients},
  author={Zhang, Zhenyu and Jaiswal, Ajay and Yin, Lu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2407.08296},
      abbr = {Preprint},
  year={2024}
}

@article{jaiswal2024galore,
  title={From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients},
  author={Jaiswal, Ajay and Yin, Lu and Zhang, Zhenyu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2407.11239},
        abbr = {Preprint},
  year={2024}
}

@article{zhang2024q,
  title={Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache},
  author={Zhang, Zhenyu and Liu, Shiwei and Chen, Runjin and Kailkhura, Bhavya and Chen, Beidi and Wang, Atlas},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={381--394},
        abbr = {MLSys2024},
  year={2024}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  booktitle={Forty-first International Conference on Machine Learning},
          abbr = {ICML2024},
              selected={true},
  year={2024}
}

@inproceedings{zhangcam,
  title={CaM: Cache Merging for Memory-efficient LLMs Inference},
  author={Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  booktitle={Forty-first International Conference on Machine Learning},
            abbr = {ICML2024},
              year={2024}
}

@inproceedings{yinjunk,
  title={Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $$\backslash$textit $\{$Irreversibly$\}$ $ and $$\backslash$textit $\{$Monotonically$\}$ $ Impairs``Difficult" Downstream Tasks in LLMs},
  author={Yin, Lu and JAISWAL, AJAY KUMAR and Liu, Shiwei and Kundu, Souvik and Wang, Zhangyang},
  booktitle={Forty-first International Conference on Machine Learning},
              abbr = {ICML2024},
              year={2024}
}

@article{xiao2024dynamic,
  title={Dynamic Data Pruning for Automatic Speech Recognition},
  author={Xiao, Qiao and Ma, Pingchuan and Fernandez-Lopez, Adriana and Wu, Boqian and Yin, Lu and Petridis, Stavros and Pechenizkiy, Mykola and Pantic, Maja and Mocanu, Decebal Constantin and Liu, Shiwei},
  journal={arXiv preprint arXiv:2406.18373},
   abbr = {Interspeech2024},
  year={2024}
}

@article{fernandez2024msrs,
  title={MSRS: Training Multimodal Speech Recognition Models from Scratch with Sparse Mask Optimization},
  author={Fernandez-Lopez, Adriana and Chen, Honglie and Ma, Pingchuan and Yin, Lu and Xiao, Qiao and Petridis, Stavros and Liu, Shiwei and Pantic, Maja},
  journal={arXiv preprint arXiv:2406.17614},
     abbr = {Interspeech2024},
  year={2024}
}

@article{zhang2023dynamic,
  title={Dynamic sparse no training: Training-free fine-tuning for sparse llms},
  author={Zhang, Yuxin and Zhao, Lirui and Lin, Mingbao and Sun, Yunyun and Yao, Yiwu and Han, Xingjia and Tanner, Jared and Liu, Shiwei and Ji, Rongrong},
  journal={arXiv preprint arXiv:2310.08915},
       abbr = {ICLR2024},
  year={2024}
}

@article{yang2023adamerging,
  title={Adamerging: Adaptive model merging for multi-task learning},
  author={Yang, Enneng and Wang, Zhenyi and Shen, Li and Liu, Shiwei and Guo, Guibing and Wang, Xingwei and Tao, Dacheng},
  journal={arXiv preprint arXiv:2310.02575},
        abbr = {ICLR2024},
  year={2024}
}

@article{liu2023don,
  title={Donâ€™t be so dense: Sparse-to-sparse gan training without sacrificing performance},
  author={Liu, Shiwei and Tian, Yuesong and Chen, Tianlong and Shen, Li},
  journal={International Journal of Computer Vision},
  volume={131},
  number={10},
  pages={2635--2648},
  year={2023},
  publisher={Springer},
        abbr = {IJCV},
}

@article{liu2022more,
  title={More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and K{\"a}rkk{\"a}inen, Tommi and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2207.03620},
  abbr = {ICLR2023},
  selected={true},
  year={2023}
}

@article{hoang2023revisiting,
  title={Revisiting pruning at initialization through the lens of ramanujan graph},
  author={Hoang, Duc NM and Liu, Shiwei and  Marculescu, Radu and Wang Zhangyang},
  abbr = {ICLR2023},
  year={2023}
}

@article{chen2023sparse,
  title={Sparse moe as the new dropout: Scaling dense and self-slimmable transformers},
  author={Chen, Tianlong and Zhang, Zhenyu and Jaiswal, Ajay and Liu, Shiwei and Wang, Zhangyang},
      abbr = {ICLR2023},
        year={2023}
}

@article{liu2023sparsity,
  title={Sparsity may cry: Let us fail (current) sparse neural networks together!},
  author={Liu, Shiwei and Chen, Tianlong and Zhang, Zhenyu and Chen, Xuxi and Huang, Tianjin and Jaiswal, Ajay and Wang, Zhangyang},
      abbr = {ICLR2023},
             year={2023}
}

@article{huang2022you,
  title={You can have better graph neural networks by not training weights at all: Finding untrained gnns tickets},
  author={Huang, Tianjin and Chen, Tianlong and Fang, Meng and Menkovski, Vlado and Zhao, Jiaxu and Yin, Lu and Pei, Yulong and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola and others},
        abbr = {LoG2022},
        selected={true},
  year={2022}
}

@article{liu2022unreasonable,
  title={The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:2202.02643},
     abbr = {ICLR2022},
     selected={true},
  year={2022}
}

@article{liu2021deep,
  title={Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity},
  author={Liu, Shiwei and Chen, Tianlong and Atashgahi, Zahra and Chen, Xiaohan and Sokar, Ghada and Mocanu, Elena and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={arXiv preprint arXiv:2106.14568},
   abbr = {ICLR2022},
  year={2022}
}

@article{liu2021sparse,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9908--9922},
  selected={true},
    abbr = {NeurIPS2021},
  year={2021}
}

@inproceedings{liu2021we,
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  booktitle={International Conference on Machine Learning},
  pages={6989--7000},
  year={2021},
    abbr = {ICML2021},
    selected={true},
  organization={PMLR}
}

@inproceedings{liu2021selfish,
  title={Selfish sparse rnn training},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
  booktitle={International Conference on Machine Learning},
  pages={6893--6904},
  year={2021},
     abbr = {ICML2021},
  organization={PMLR}
}


